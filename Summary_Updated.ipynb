{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def summary(text):\n",
    "    nlp=spacy.load('en_core_web_sm')\n",
    "    doc=nlp(text)\n",
    "    tokens=[token.text for token in doc]\n",
    "    stopwords=list(STOP_WORDS)\n",
    "    word_frequencies={}\n",
    "    for word in doc:\n",
    "        if word.text.lower() not in stopwords:\n",
    "            if word.text.lower() not in punctuation:\n",
    "                if word.text not in word_frequencies.keys():\n",
    "                    word_frequencies[word.text] = 1\n",
    "                else:\n",
    "                    word_frequencies[word.text]+=1\n",
    "    max_frequency=max(word_frequencies.values())\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word]=word_frequencies[word]/max_frequency\n",
    "    sentence_tokens=[sent for sent in doc.sents]\n",
    "    sentence_scores={}\n",
    "    for sent in sentence_tokens:\n",
    "        for word in sent:\n",
    "            if word.text.lower() in word_frequencies.keys():\n",
    "                if sent not in sentence_scores.keys():\n",
    "                    sentence_scores[sent] = word_frequencies[word.text.lower()]\n",
    "                else:\n",
    "                    sentence_scores[sent] += word_frequencies[word.text.lower()]\n",
    "    select_length=int(len(sentence_tokens)*0.3)\n",
    "    summary1=nlargest(select_length,sentence_scores,key=sentence_scores.get)\n",
    "    print(\"Summary using NLP\")\n",
    "#     type(summary1)\n",
    "#     print(summary1)\n",
    "    result=\"\"\n",
    "    for s in summary1:\n",
    "        result=result+s.text\n",
    "    print(result)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #T5\n",
    "    model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "    tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "    preprocess_text = text.strip().replace(\"\\n\",\"\")\n",
    "    t5_prepared_Text = \"summarize: \"+preprocess_text\n",
    "   # print (\"original text preprocessed: \\n\", preprocess_text)\n",
    "    tokenized_text = tokenizer.encode(t5_prepared_Text, return_tensors=\"pt\")\n",
    "    summary_ids = model.generate(tokenized_text,\n",
    "                                        num_beams=4,\n",
    "                                        no_repeat_ngram_size=2,\n",
    "                                        min_length=30,\n",
    "                                        max_length=100,\n",
    "                                        early_stopping=True)\n",
    "\n",
    "    output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    print (\"\\n\\nSummarized text using Model: \\n\",output)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Bleu Score\n",
    "    reference=output.split(\" \")\n",
    "    candidate=result.split(\" \")\n",
    "    score = sentence_bleu(reference, candidate)\n",
    "   # print(\"Using Spacy\")\n",
    "   # print(result)\n",
    "    print('\\n')\n",
    "   # print(\"Using T5\")\n",
    "   # print(output)\n",
    "    print(score)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "     \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"Nationalism happens because of common factors. The people of a nation share these common factors. These common factors are common language, history, culture, traditions, mentality, and territory. Thus a sense of belonging would certainly come in people. It would inevitably happen, whether you like it or not. Therefore, a feeling of unity and love would happen among national citizens. In this way, Nationalism gives strength to the people of the nation.\n",
    "\n",
    "Nationalism has an inverse relationship with crime. It seems like crime rates are significantly lower in countries with strong Nationalism. This happens because Nationalism puts feelings of love towards fellow countrymen. Therefore, many people avoid committing a crime against their own countrymen. Similarly, corruption is also low in such countries. Individuals in whose heart is Nationalism, avoid corruption. This is because they feel guilty to harm their country.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "lecture on phases of compiler the objective of the letter is to inter compilation and then we try to relate them to language construct the language constructs you might have already studied in theory of computation so this brief overview of compiler was already discussed in the previous lecture in this lecture will go through each of the phases in compiler and try to explore its details so lexical analysis is the first page of compilation process and this lexical analyser is sometimes also called as Kana now why scanner because this is the only face with gets to interact with the source program should be source program here is nothing but our high level language program se for example if there is \n",
    "a c compiler then the source program would be a high level language program which is written in C language and as well as it get it gets into the compiler it first goes to the lexical analyser also called as camp with scans the input from the C program for hence it is also called as I can see why it is called as a lexical analyser so it is the interface of the compiler to the outside world this is the only phase which receives the input from the C program to get the source program which is safe for example in C or any high level language the lexical analyser stars is to give a stream of tokens Nabi tokens are basically representation of logically cohesive sequence of characters to be sequence of tokens are also referred to as leg zines hence the name of the sev is lexical analysis the water token can we for any specific language it depends on the line image which is considered so some common examples of tokens are listed here so keywords from the language can be the example of tokens operators identify all symbols constant from string with you take as input all these are type of tokens so basically these are the vocabulary of any language write so whenever you try to learn a new language what what are the keywords in this language for tour operators you can use what all are the rules to generate the name for a identify identify everything but a variable or a variable name with you give symbols various symbols which are allowed and there could be some constant numbers in the program there could be some strings se printer statements have string inside have some text inside \"that is referred to as strings so what does a lexical analyser do it scans the input source program identify the valid words from the language what could be there is valid words this can be any one of these which are listed it also removes the extra white spaces while writing the program is then you might give some extra blank spaces you also provide certain comments so all these things are not meaningful to the program right so all these are removed and only the important words which are Koi called as tokens for leg zines which belong to that language would be extracted now generally a lexical analyser is implemented as a finite automata so just to remind you what a finite automata is you start from a starting state this se start state is is 0 and then if you get some input se you go and proceed to some others date and this goes on until you reach se final state that is to accept any token or to do tokenization we need to implement a finite automata so lexical analyser in just some of what a lexical analyser Does It Breaks up the source program into tokens what are tokens tokens could be keyword operator identify anything and how is it done flood know whether the incoming now everything is scanned a character by \n",
    "character se if I have in the program void so it will first can we then over then I then the and then comes up space after it Encounters BOI ID and then it gets to know of space is there it knows ok before space the world was wide and it then checks whether wide can be part of anything any token which is specified in the language yes why I recognise wide is a reserved word or maybe it can be simply called as a Keyword because it has a reserved meaning in a language to avoid is used in the sea so I'll show you an example here \n",
    "if this is the input program given to the lexical analyser I have written a small C program with print the numbers from 1 to 10 and you give it to the lexical analyser and just \n",
    "want to see what will be the output of it so the output would be every single token is identified and now something has to be done with these tokens will see what has to be done so that the spot the video for a moment and try to count how many tokens will be there and what these tokens will be so wide is one main is one opening (closing) {in account; all these are different tokens less than equal to 10 is a a separate token; is a separate tokens of all these tokens would be identified this is the work of a lexical analyser using all these tokens it stores this in a symbol table what does in the symbol table would contain wheels talk about a few entries related on will discuss the symbol table in detail so what it does it create a tuple entry of the typetoken type and attribute Kari so if we consider a statement se in account is equal to one and then there is a seneca taking the first world for the token here is in token type so what type of token is in it it is basically a reserved word for a ki world now what will the symbol table entry be the symbol table entries token type fat open type here is keyword and the attribute value the actual value of this token which is in next word is account so what is count count is not a reserved word it is not an operator not a constant it is basically some variable which the programmer might have created this account is nothing but a variable should be referred to the variable as an identifier what will be the symbol table entry it would be identified have used a small form of identified as ID generally will be using ID to denote the identifier and now hear the symbol table entry was in this ain't is already defined but we haven't define count it right so count would be basically a variable which is allotted some place so this will be the second entry here the attribute value would be the pointer to that memory location this is a memory location where we have the account back \n",
    "probably continue with this video as the second part in the next lecture\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"\n",
    "what does in the symbol table would contain wheels talk about a few entries related on will discuss the symbol table in detail so what it does it create a tuple entry of the typetoken type and attribute Kari so if we consider a statement se in account is equal to one and then there is a seneca taking the first world for the token here is in token type so what type of token is in it it is basically a reserved word for a ki world now what will the symbol table entry be the symbol table entries token type fat open type here is \n",
    "keyword and the attribute value the actual value of this token which is in next word is account so what is count count is not a reserved word it is not an operator not a constant it is basically some variable which the programmer might have created this account is nothing but a variable should be referred to the variable as an identifier what will be the symbol table entry it would be identified have used a small form of identified as ID generally will be using ID to denote the identifier and now hear the symbol table entry was in this ain't is already defined but we haven't define count it right so count would be basically a variable which is allotted some place so this will be the second entry here the attribute value would be the pointer to that memory location this is a memory location where we have the account back probably continue with this video as the second part \n",
    "in the next lecture\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary using NLP\n",
      "\n",
      "lecture on phases of compiler the objective of the letter is to inter compilation and then we try to relate them to language construct the language constructs you might have already studied in theory of computation so this brief overview of compiler was already discussed in the previous lecture in this lecture will go through each of the phases in compiler and try to explore its details so lexical analysis is the first page of compilation process and this lexical analyser is sometimes also called as Kana now why scanner because this is the only face with gets to interact with the source program should be source program here is nothing but our high level language program se for example if there is \n",
      "a c compiler then the source program would be a high level language program which is written in C language and as well as it get it gets into the compiler it first goes to the lexical analyser also called as camp with scans the input from the C program for hence it is also called as I can see why it is called as a lexical analyser so it is the interface of the compiler to the outside world this is the only phase which receives the input from the C program to get the source program which is safe for example in C or any high level language the lexical analyser stars is to give a stream of tokens Nabi tokens are basically representation of logically cohesive sequence of characters to be sequence of tokens are also referred to as leg zines hence the name of the sev is lexical analysis the water tokenso I'll show you an example here \n",
      "if this is the input program given to the lexical analyser I have written a small C program with print the numbers from 1 to 10 and you give it to the lexical analyser and just \n",
      "want to see what will be the output of it so the output would be every single token is identified and now something has to be done with these tokens will see what has to be done so that the spot the video for a moment and try to count how many tokens will be there and what these tokens will be so wide is one main is one opening (closing) {in account; all these are different tokens less than equal to 10 is a a separate token; is a separate tokens of all these tokens would be identified this is the work of a lexical analyser using all these tokens it stores this in a symbol table what does in the symbol table would contain wheels talk about a few entries related on will discuss the symbol table in detail so what it does it create a tuple entry of the typetoken type and attribute Kariso whenever you try to learn a new language what what are the keywords in this language for tour operators you can use what all are the rules to generate the name for a identify identify everything but a variable or a variable name with you give symbols various symbols which are allowed and there could be some constant numbers in the program there could be some strings se printer statements have string inside have some text inside \"that is referred to as strings so what does a lexical analyser do it scans the input source program identify the valid words from the language what could be there is valid words this can be any one of these which are listed it also removes the extra white spaces while writing the program is then you might give some extra blank spaces you also provide certain comments so all these things are not meaningful to the programand then if you get some input se you go and proceed to some others date and this goes on until you reach se final state that is to accept any token or to do tokenization we need to implement a finite automata so lexical analyser in just some of what a lexical analyser Does It Breaks up the source program into tokens what are tokens tokens could be keyword operator identify anything and how is it done flood know whether the incoming now everything is scanned a character by \n",
      "character se if I have in the program void so it will first can we then over then\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1361 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Summarized text using Model: \n",
      " lexical analysis is the first page of compilation process. this brief overview of compiler will go through each of the phases in compiler so we try to relate them to language constructs you might have already studied in theory of computation. the next lecture will be on the topic of how to use a finite automata to measure the number of tokens to be scanned in the language.\n",
      "\n",
      "\n",
      "4.939868538227858e-232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python384\\lib\\site-packages\\nltk\\translate\\bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\python\\python384\\lib\\site-packages\\nltk\\translate\\bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\python\\python384\\lib\\site-packages\\nltk\\translate\\bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "summary(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary using NLP\n",
      "[These common factors are common language, history, culture, traditions, mentality, and territory., The people of a nation share these common factors., Therefore, many people avoid committing a crime against their own countrymen., It seems like crime rates are significantly lower in countries with strong Nationalism.]\n",
      "\n",
      "\n",
      "Summarized text using Model: \n",
      " nationalism has an inverse relationship with crime. it seems like crime rates are significantly lower in countries with strong Nationalism. this happens because nationalism puts feelings of love towards fellow countrymen. corruption is also low in such countries.\n",
      "\n",
      "\n",
      "8.510469113101058e-232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
